{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4701735,"sourceType":"datasetVersion","datasetId":2627880},{"sourceId":9737324,"sourceType":"datasetVersion","datasetId":5959750}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\ntrain_file = \"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\"\ndev_file = \"/kaggle/input/vietnamese-squad/dev-v2.0-translated.json\"\nwith open(train_file, \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\n\nwith open(dev_file, \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:29:41.239844Z","iopub.execute_input":"2024-10-27T15:29:41.240561Z","iopub.status.idle":"2024-10-27T15:29:43.730531Z","shell.execute_reply.started":"2024-10-27T15:29:41.240521Z","shell.execute_reply":"2024-10-27T15:29:43.729734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:29:43.732284Z","iopub.execute_input":"2024-10-27T15:29:43.732657Z","iopub.status.idle":"2024-10-27T15:29:43.738839Z","shell.execute_reply.started":"2024-10-27T15:29:43.732615Z","shell.execute_reply":"2024-10-27T15:29:43.737881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_format(data):\n    json_data = []\n    for d in data:\n        sample = {\"context\": d[0],\n                \"question\": d[1],\n                \"ans\": d[2]}\n\n        json_data.append(sample)\n    return json_data\n\ntrain_data = change_format(train_data)\ndev_data = change_format(dev_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:41:05.959426Z","iopub.execute_input":"2024-10-27T15:41:05.959789Z","iopub.status.idle":"2024-10-27T15:41:06.065062Z","shell.execute_reply.started":"2024-10-27T15:41:05.959756Z","shell.execute_reply":"2024-10-27T15:41:06.064078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:51:44.091108Z","iopub.execute_input":"2024-10-27T15:51:44.091536Z","iopub.status.idle":"2024-10-27T15:51:44.097655Z","shell.execute_reply.started":"2024-10-27T15:51:44.091492Z","shell.execute_reply":"2024-10-27T15:51:44.096730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndef create_segment_id(tokenized_input):\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = tokenized_input['input_ids'].index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(tokenized_input['input_ids']) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(tokenized_input['input_ids'])\n    \n    return segment_ids","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:53:19.772246Z","iopub.execute_input":"2024-10-27T15:53:19.772716Z","iopub.status.idle":"2024-10-27T15:53:25.188245Z","shell.execute_reply.started":"2024-10-27T15:53:19.772672Z","shell.execute_reply":"2024-10-27T15:53:25.187418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef find_start_end_char(ques, answer, context):\n    match = re.search(re.escape(answer), context, re.IGNORECASE)\n    if match:\n        start_char = match.start()\n        end_char = match.end()\n    else:\n        start_char, end_char = -1, -1\n    return start_char, end_char ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:53:44.061909Z","iopub.execute_input":"2024-10-27T15:53:44.062429Z","iopub.status.idle":"2024-10-27T15:53:44.068084Z","shell.execute_reply.started":"2024-10-27T15:53:44.062392Z","shell.execute_reply":"2024-10-27T15:53:44.067211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport wandb\n\nwith open('/kaggle/input/wandb-key/wandb_key.txt', 'r') as f:\n    os.environ['WANDB_API_KEY'] = f.read().strip()\n    print(f.read().strip())\n\n# Initialize wandb without interactive login\nwandb.login(key=os.environ['WANDB_API_KEY'])","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:44:23.800566Z","iopub.execute_input":"2024-10-27T10:44:23.800832Z","iopub.status.idle":"2024-10-27T10:44:24.115368Z","shell.execute_reply.started":"2024-10-27T10:44:23.800802Z","shell.execute_reply":"2024-10-27T10:44:24.114344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom transformers import BertForQuestionAnswering\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\nmodel = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n\ndef preprocess_function(example):\n    inputs = tokenizer(\n        example[\"question\"], \n        example[\"context\"], \n        max_length=128, \n        padding=\"max_length\",\n        truncation=True,\n        return_offsets_mapping=True\n    )\n    \n    start_char, end_char = find_start_end_char(example['question'], example[\"ans\"], example[\"context\"])\n    if start_char == -1 or end_char == -1:\n        inputs[\"start_positions\"], inputs[\"end_positions\"] = -1, -1\n    else:     \n        # Convert character positions to token positions\n        offsets = inputs[\"offset_mapping\"]\n        inputs[\"start_positions\"], inputs[\"end_positions\"] = 0, 0\n        for idx, (start, end) in enumerate(offsets):\n            if start == start_char:\n                inputs[\"start_positions\"] = idx\n            if end == end_char:\n                inputs[\"end_positions\"] = idx\n                break\n        inputs.pop(\"offset_mapping\")  # Remove offset_mapping after use\n    \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:09:41.371646Z","iopub.execute_input":"2024-10-27T16:09:41.372150Z","iopub.status.idle":"2024-10-27T16:09:56.285236Z","shell.execute_reply.started":"2024-10-27T16:09:41.372113Z","shell.execute_reply":"2024-10-27T16:09:56.284426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \nfrom datasets import Dataset\n\ndef tokenize_data(squad_formatted_data):\n    \n    tokenized_dataset = []\n    for item in tqdm(squad_formatted_data):\n        tokenized_input = preprocess_function(item)\n        segment_ids = create_segment_id(tokenized_input)\n        tokenized_input[\"token_type_ids\"] = segment_ids\n        tokenized_dataset.append(tokenized_input)\n    \n    return tokenized_dataset\n\n# Tokenize and add custom segment_ids\ntokenized_train_data = tokenize_data(train_data)\ntokenized_dev_data = tokenize_data(dev_data)\n\n# Convert to Dataset    \ntokenized_train_data = Dataset.from_list(tokenized_train_data)\ntokenized_dev_data = Dataset.from_list(tokenized_dev_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:10:07.174291Z","iopub.execute_input":"2024-10-27T16:10:07.175398Z","iopub.status.idle":"2024-10-27T16:12:55.636836Z","shell.execute_reply.started":"2024-10-27T16:10:07.175354Z","shell.execute_reply":"2024-10-27T16:12:55.635784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding, Trainer, TrainingArguments, BertForQuestionAnswering\n\n\n# Use the tokenizer you used in preprocess_function\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",           # Evaluate and save at the end of each epoch\n    save_strategy=\"epoch\",                 # Save model at each evaluation step\n    load_best_model_at_end=True,           # Load the best model at the end of training\n    metric_for_best_model=\"eval_loss\",     # Metric to determine the best model (can adjust as needed)\n    greater_is_better=False,               # Set to False if lower is better (e.g., for loss)\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=25,\n    weight_decay=0.01,\n    fp16=True,                             # Use mixed precision for memory efficiency\n    gradient_accumulation_steps=4,         # Simulate larger batch size\n    save_total_limit=1                     # Keep only the best checkpoint\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_data,\n    eval_dataset=tokenized_dev_data,  # replace with a validation set if available\n    data_collator=data_collator,\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:13:19.837913Z","iopub.execute_input":"2024-10-27T16:13:19.838534Z","iopub.status.idle":"2024-10-27T16:32:16.046100Z","shell.execute_reply.started":"2024-10-27T16:13:19.838494Z","shell.execute_reply":"2024-10-27T16:32:16.044255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eval model","metadata":{}},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n\n# Save the model and tokenizer\nmodel.save_pretrained(\"./fine_tuned_xlm-roberta\")\ntokenizer.save_pretrained(\"./fine_tuned_xlm-roberta\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:49:02.130259Z","iopub.status.idle":"2024-10-27T10:49:02.130620Z","shell.execute_reply.started":"2024-10-27T10:49:02.130437Z","shell.execute_reply":"2024-10-27T10:49:02.130464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run inference","metadata":{}},{"cell_type":"code","source":"# Check if GPU is available and set the device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the appropriate device\nmodel.to(device)\n\ndef answer_question(context, question):\n    inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n   \n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n    start_idx = torch.argmax(start_scores)\n    end_idx = torch.argmax(end_scores) + 1\n\n    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:49:21.495104Z","iopub.execute_input":"2024-10-27T10:49:21.495521Z","iopub.status.idle":"2024-10-27T10:49:21.597940Z","shell.execute_reply.started":"2024-10-27T10:49:21.495482Z","shell.execute_reply":"2024-10-27T10:49:21.596798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample in dev_data[:5]:\n    \n    print(\"Answer: \", sample['ans'])\n    print(\"Predict answer: \", answer_question(sample['context'], sample['question']))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:52:40.132413Z","iopub.execute_input":"2024-10-27T10:52:40.133197Z","iopub.status.idle":"2024-10-27T10:52:40.228246Z","shell.execute_reply.started":"2024-10-27T10:52:40.133158Z","shell.execute_reply":"2024-10-27T10:52:40.227308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}